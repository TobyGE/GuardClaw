# Using LM Studio with GuardClaw

GuardClaw æ”¯æŒä½¿ç”¨æœ¬åœ° LLM è¿›è¡Œå‘½ä»¤å®‰å…¨åˆ†æï¼Œæ— éœ€è°ƒç”¨äº‘ç«¯ APIï¼

## è®¾ç½® LM Studio

### 1. ä¸‹è½½å¹¶å®‰è£… LM Studio

- å®˜ç½‘ï¼š<https://lmstudio.ai/>
- æ”¯æŒ macOS, Windows, Linux
- å…è´¹ä½¿ç”¨

### 2. ä¸‹è½½æ¨èæ¨¡å‹

åœ¨ LM Studio ä¸­æœç´¢å¹¶ä¸‹è½½ï¼š

**æ¨èæ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½æ’åºï¼‰ï¼š**

**æœ€ä½³æ€§èƒ½ï¼š**

- `TheBloke/Mistral-7B-Instruct-v0.2-GGUF` (7B, ~4GB)
- `TheBloke/Llama-2-13B-Chat-GGUF` (13B, ~8GB)

**å¹³è¡¡é€‰æ‹©ï¼š**

- `TheBloke/Phi-2-GGUF` (2.7B, ~2GB) - è½»é‡ä½†æ™ºèƒ½
- `TheBloke/TinyLlama-1.1B-Chat-GGUF` (1.1B, ~1GB) - è¶…å¿«é€Ÿ

**æœ€å¼ºå¤§ï¼ˆéœ€è¦å¥½æ˜¾å¡ï¼‰ï¼š**

- `TheBloke/CodeLlama-34B-Instruct-GGUF` (34B, ~20GB)
- `TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF` (47B, ~26GB)

**æç¤ºï¼š** ä¸‹è½½ `Q4_K_M` æˆ– `Q5_K_M` é‡åŒ–ç‰ˆæœ¬ï¼Œå¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ã€‚

### 3. å¯åŠ¨ LM Studio Server

1. åœ¨ LM Studio ä¸­åŠ è½½æ¨¡å‹
2. ç‚¹å‡» **"Local Server"** æ ‡ç­¾
3. ç‚¹å‡» **"Start Server"**
4. é»˜è®¤è¿è¡Œåœ¨ `http://localhost:1234`

### 4. é…ç½® GuardClaw

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```bash
# ä½¿ç”¨ LM Studio åç«¯
SAFEGUARD_BACKEND=lmstudio

# LM Studio é…ç½®
LMSTUDIO_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=auto
```

### 5. é‡å¯ GuardClaw

```bash
npm start
```

## æµ‹è¯•åˆ†æ

å¯åŠ¨åï¼ŒGuardClaw ä¼šè‡ªåŠ¨ä½¿ç”¨ LM Studio åˆ†æå‘½ä»¤ï¼š

```bash
# åœ¨ GuardClaw Dashboard ä¸­è§‚å¯Ÿè¿™äº›å‘½ä»¤çš„é£é™©è¯„åˆ†ï¼š

# å®‰å…¨å‘½ä»¤
ls -la ~/documents        # åº”è¯¥æ˜¯ 1-2/10 (safe)
cat README.md             # åº”è¯¥æ˜¯ 1/10 (safe)

# ä¸­ç­‰é£é™©
rm old-file.txt           # åº”è¯¥æ˜¯ 5-6/10 (warning)
curl https://api.com      # åº”è¯¥æ˜¯ 3-4/10 (safe-warning)

# é«˜é£é™©
rm -rf /tmp/*            # åº”è¯¥æ˜¯ 7-8/10 (danger)
sudo rm -rf /            # åº”è¯¥æ˜¯ 10/10 (blocked)
```

## æ•…éšœæ’æŸ¥

### LM Studio è¿æ¥å¤±è´¥

**é”™è¯¯ï¼š** `LM Studio analysis failed: fetch failed`

**è§£å†³æ–¹æ³•ï¼š**

1. ç¡®è®¤ LM Studio Server æ­£åœ¨è¿è¡Œï¼ˆç»¿è‰² "Running" æ ‡å¿—ï¼‰
2. æ£€æŸ¥ç«¯å£ï¼šé»˜è®¤ `1234`
3. æµ‹è¯•è¿æ¥ï¼š

```bash
curl http://localhost:1234/v1/models
```

### æ¨¡å‹å“åº”å¤ªæ…¢

**é—®é¢˜ï¼š** å‘½ä»¤åˆ†æéœ€è¦ 5-10 ç§’

**è§£å†³æ–¹æ³•ï¼š**

1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆPhi-2, TinyLlamaï¼‰
2. æ£€æŸ¥ CPU/GPU ä½¿ç”¨ç‡
3. åœ¨ LM Studio ä¸­è°ƒæ•´ `Context Length` å’Œ `GPU Layers`

### JSON è§£æå¤±è´¥

**é”™è¯¯ï¼š** `Failed to parse response`

**è§£å†³æ–¹æ³•ï¼š**

1. ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼ˆInstruct variantsï¼‰
2. åœ¨ LM Studio è®¾ç½®ä¸­é™ä½ `Temperature` (0.1-0.3)
3. GuardClaw ä¼šè‡ªåŠ¨ fallback åˆ°æ¨¡å¼åŒ¹é…

## æ€§èƒ½å¯¹æ¯”

| Backend                    | å»¶è¿Ÿ   | å‡†ç¡®åº¦     | æˆæœ¬ | éšç§     |
| -------------------------- | ------ | ---------- | ---- | -------- |
| **LM Studio (Mistral 7B)** | ~1-2s  | â­â­â­â­   | å…è´¹ | ğŸ”’ æœ¬åœ° |
| **LM Studio (Phi-2)**      | ~0.5s  | â­â­â­     | å…è´¹ | ğŸ”’ æœ¬åœ° |
| **Claude API**             | ~0.3s  | â­â­â­â­â­ | $$   | â˜ï¸ äº‘ç«¯ |
| **Fallback (pattern)**     | ~0.01s | â­â­       | å…è´¹ | ğŸ”’ æœ¬åœ° |

## å…¶ä»–æœ¬åœ°é€‰é¡¹

### Ollama

```bash
# .env
SAFEGUARD_BACKEND=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# å®‰è£… Ollama: https://ollama.ai/
ollama run llama3
```

### è‡ªæ‰˜ç®¡ API

GuardClaw å…¼å®¹ä»»ä½• OpenAI-compatible API ç«¯ç‚¹ã€‚

## æ¨èé…ç½®

**å¼€å‘ç¯å¢ƒï¼ˆé€Ÿåº¦ä¼˜å…ˆï¼‰ï¼š**

```bash
SAFEGUARD_BACKEND=lmstudio
LMSTUDIO_MODEL=phi-2
```

**ç”Ÿäº§ç¯å¢ƒï¼ˆå‡†ç¡®åº¦ä¼˜å…ˆï¼‰ï¼š**

```bash
SAFEGUARD_BACKEND=lmstudio
LMSTUDIO_MODEL=mistral-7b-instruct
```

**ä½èµ„æºè®¾å¤‡ï¼š**

```bash
SAFEGUARD_BACKEND=fallback  # ä½¿ç”¨æ¨¡å¼åŒ¹é…
```

## æç¤º

1. **é¦–æ¬¡åŠ è½½æ…¢**ï¼šç¬¬ä¸€æ¬¡åˆ†æä¼šåŠ è½½æ¨¡å‹ï¼Œéœ€è¦ 10-30 ç§’
2. **æ‰¹é‡åˆ†æ**ï¼šLM Studio æ”¯æŒå¹¶å‘è¯·æ±‚
3. **å¤šæ¨¡å‹åˆ‡æ¢**ï¼šå¯ä»¥åœ¨ LM Studio ä¸­å¿«é€Ÿåˆ‡æ¢æ¨¡å‹æµ‹è¯•æ•ˆæœ
4. **GPU åŠ é€Ÿ**ï¼šåœ¨ LM Studio è®¾ç½®ä¸­å¯ç”¨ GPU offloading å¯æ˜¾è‘—æé€Ÿ

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** æ£€æŸ¥ GuardClaw æ—¥å¿—ï¼š

```bash
tail -f ~/clawd/guardclaw/guardclaw.log
```
