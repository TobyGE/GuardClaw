# Using LM Studio with GuardClaw

GuardClaw æ”¯æŒä½¿ç”¨æœ¬åœ° LLM è¿›è¡Œå‘½ä»¤å®‰å…¨åˆ†æï¼Œæ— éœ€è°ƒç”¨äº‘ç«¯ APIï¼

**ğŸ” å¯åŠ¨æ—¶è‡ªåŠ¨æ‰«æï¼š** GuardClaw ä¼šåœ¨å¯åŠ¨æ—¶ä¸»åŠ¨æµ‹è¯• Gateway å’Œ LM Studio çš„è¿æ¥çŠ¶æ€ï¼Œå¹¶æ˜¾ç¤ºè¯¦ç»†çš„è¿æ¥ä¿¡æ¯ã€‚

## è®¾ç½® LM Studio

### 1. ä¸‹è½½å¹¶å®‰è£… LM Studio

- å®˜ç½‘ï¼š<https://lmstudio.ai/>
- æ”¯æŒ macOS, Windows, Linux
- å…è´¹ä½¿ç”¨

### 2. ä¸‹è½½æ¨èæ¨¡å‹

åœ¨ LM Studio ä¸­æœç´¢å¹¶ä¸‹è½½ï¼š

**æ¨èæ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½æ’åºï¼‰ï¼š**

**æœ€ä½³æ€§èƒ½ï¼š**

- `TheBloke/Mistral-7B-Instruct-v0.2-GGUF` (7B, ~4GB)
- `TheBloke/Llama-2-13B-Chat-GGUF` (13B, ~8GB)

**å¹³è¡¡é€‰æ‹©ï¼š**

- `TheBloke/Phi-2-GGUF` (2.7B, ~2GB) - è½»é‡ä½†æ™ºèƒ½
- `TheBloke/TinyLlama-1.1B-Chat-GGUF` (1.1B, ~1GB) - è¶…å¿«é€Ÿ

**æœ€å¼ºå¤§ï¼ˆéœ€è¦å¥½æ˜¾å¡ï¼‰ï¼š**

- `TheBloke/CodeLlama-34B-Instruct-GGUF` (34B, ~20GB)
- `TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF` (47B, ~26GB)

**æç¤ºï¼š** ä¸‹è½½ `Q4_K_M` æˆ– `Q5_K_M` é‡åŒ–ç‰ˆæœ¬ï¼Œå¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ã€‚

### 3. å¯åŠ¨ LM Studio Server

1. åœ¨ LM Studio ä¸­åŠ è½½æ¨¡å‹
2. ç‚¹å‡» **"Local Server"** æ ‡ç­¾
3. ç‚¹å‡» **"Start Server"**
4. é»˜è®¤è¿è¡Œåœ¨ `http://localhost:1234`

### 4. é…ç½® GuardClaw

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```bash
# ä½¿ç”¨ LM Studio åç«¯
SAFEGUARD_BACKEND=lmstudio

# LM Studio é…ç½®
LMSTUDIO_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=auto
```

### 5. é‡å¯ GuardClaw

```bash
npm start
```

å¯åŠ¨æ—¶ï¼Œä½ åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š

```
ğŸ”Œ Connecting to Clawdbot Gateway...
   URL: ws://127.0.0.1:18789

âœ… Connected successfully!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ›¡ï¸  Safeguard: LMSTUDIO

ğŸ” Testing LLM backend connection...
âœ… LMSTUDIO: Connected (1 model loaded)
   ğŸ“¦ Models: mistral-7b-instruct-v0.2

ğŸ” Fetching Gateway information...
âœ… Gateway Status:
   ğŸ“Š Active Sessions: 2
   ğŸ¤– Agents:
      - agent:main:main (main) - last active: 3:45:23 PM
      - agent:helper:sub (helper) - last active: 3:42:15 PM

ğŸ¯ GuardClaw is now monitoring your agents!
```

**å¯åŠ¨æ—¶ä¸»åŠ¨æ‰«æçš„å†…å®¹ï¼š**

1. **Gateway è¿æ¥** - å»ºç«‹ WebSocket è¿æ¥
2. **LM Studio è¿æ¥** - æµ‹è¯•æœ¬åœ° LLM å¯ç”¨æ€§å’Œå·²åŠ è½½çš„æ¨¡å‹
3. **æ´»è·ƒä¼šè¯** - è·å–å½“å‰è¿è¡Œçš„ agent åˆ—è¡¨
4. **æƒé™æ£€æŸ¥** - éªŒè¯ API è®¿é—®æƒé™

å¦‚æœ LM Studio æœªè¿è¡Œæˆ–æœªåŠ è½½æ¨¡å‹ï¼Œä¼šæ˜¾ç¤ºï¼š

```
âŒ LMSTUDIO: Failed to connect: fetch failed

ğŸ’¡ LM Studio Setup:
   1. Download and install LM Studio from https://lmstudio.ai/
   2. Load a model (recommended: Mistral-7B-Instruct or Phi-2)
   3. Start the Local Server (default: http://localhost:1234)
   4. Or set SAFEGUARD_BACKEND=fallback in .env

   GuardClaw will use pattern-matching fallback until LM Studio connects.
```

## æµ‹è¯•åˆ†æ

å¯åŠ¨åï¼ŒGuardClaw ä¼šè‡ªåŠ¨ä½¿ç”¨ LM Studio åˆ†æå‘½ä»¤ï¼š

```bash
# åœ¨ GuardClaw Dashboard ä¸­è§‚å¯Ÿè¿™äº›å‘½ä»¤çš„é£é™©è¯„åˆ†ï¼š

# å®‰å…¨å‘½ä»¤
ls -la ~/documents        # åº”è¯¥æ˜¯ 1-2/10 (safe)
cat README.md             # åº”è¯¥æ˜¯ 1/10 (safe)

# ä¸­ç­‰é£é™©
rm old-file.txt           # åº”è¯¥æ˜¯ 5-6/10 (warning)
curl https://api.com      # åº”è¯¥æ˜¯ 3-4/10 (safe-warning)

# é«˜é£é™©
rm -rf /tmp/*            # åº”è¯¥æ˜¯ 7-8/10 (danger)
sudo rm -rf /            # åº”è¯¥æ˜¯ 10/10 (blocked)
```

## å¯åŠ¨æ—¶ä¸»åŠ¨æ‰«æ

GuardClaw åœ¨å¯åŠ¨æ—¶ä¼šä¸»åŠ¨æ‰«æå¹¶æ˜¾ç¤ºä»¥ä¸‹ä¿¡æ¯ï¼š

### 1. Gateway è¿æ¥çŠ¶æ€
- WebSocket è¿æ¥æ˜¯å¦æˆåŠŸ
- è¿æ¥çš„ Gateway URL

### 2. LM Studio / LLM åç«¯çŠ¶æ€
- æ˜¯å¦å¯ä»¥è¿æ¥åˆ° LM Studio
- å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
- è¿æ¥å¤±è´¥æ—¶çš„è®¾ç½®æŒ‡å¼•

### 3. Gateway ä¿¡æ¯
- å½“å‰æ´»è·ƒçš„ session æ•°é‡
- æ­£åœ¨è¿è¡Œçš„ agent åˆ—è¡¨
- æ¯ä¸ª agent çš„æœ€åæ´»è·ƒæ—¶é—´

### 4. æƒé™æ£€æŸ¥
- æ˜¯å¦å…·æœ‰ `operator.admin` æƒé™
- æ˜¯å¦å¯ä»¥è®¿é—® `sessions.list` å’Œ `chat.history` API
- ç¼ºå°‘æƒé™æ—¶ä¼šæç¤ºå¦‚ä½•é…ç½®

è¿™æ ·ä½ åœ¨å¯åŠ¨æ—¶å°±èƒ½ç«‹å³çŸ¥é“ç³»ç»Ÿçš„å®Œæ•´çŠ¶æ€ï¼Œè€Œä¸éœ€è¦ç­‰åˆ°ç¬¬ä¸€æ¬¡åˆ†ææ—¶æ‰å‘ç°é—®é¢˜ã€‚

## æ•…éšœæ’æŸ¥

### LM Studio è¿æ¥å¤±è´¥

**é”™è¯¯ï¼š** `LM Studio analysis failed: fetch failed`

**è§£å†³æ–¹æ³•ï¼š**

1. ç¡®è®¤ LM Studio Server æ­£åœ¨è¿è¡Œï¼ˆç»¿è‰² "Running" æ ‡å¿—ï¼‰
2. æ£€æŸ¥ç«¯å£ï¼šé»˜è®¤ `1234`
3. æµ‹è¯•è¿æ¥ï¼š

```bash
curl http://localhost:1234/v1/models
```

### æ¨¡å‹å“åº”å¤ªæ…¢

**é—®é¢˜ï¼š** å‘½ä»¤åˆ†æéœ€è¦ 5-10 ç§’

**è§£å†³æ–¹æ³•ï¼š**

1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆPhi-2, TinyLlamaï¼‰
2. æ£€æŸ¥ CPU/GPU ä½¿ç”¨ç‡
3. åœ¨ LM Studio ä¸­è°ƒæ•´ `Context Length` å’Œ `GPU Layers`

### JSON è§£æå¤±è´¥

**é”™è¯¯ï¼š** `Failed to parse response`

**è§£å†³æ–¹æ³•ï¼š**

1. ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼ˆInstruct variantsï¼‰
2. åœ¨ LM Studio è®¾ç½®ä¸­é™ä½ `Temperature` (0.1-0.3)
3. GuardClaw ä¼šè‡ªåŠ¨ fallback åˆ°æ¨¡å¼åŒ¹é…

## æ€§èƒ½å¯¹æ¯”

| Backend                    | å»¶è¿Ÿ   | å‡†ç¡®åº¦ | æˆæœ¬ | éšç§ |
| -------------------------- | ------ | ------ | ---- | ---- |
| **LM Studio (Mistral 7B)** | ~1-2s  | 4/5    | å…è´¹ | æœ¬åœ° |
| **LM Studio (Phi-2)**      | ~0.5s  | 3/5    | å…è´¹ | æœ¬åœ° |
| **Claude API**             | ~0.3s  | 5/5    | $$   | äº‘ç«¯ |
| **Fallback (pattern)**     | ~0.01s | 2/5    | å…è´¹ | æœ¬åœ° |

## å…¶ä»–æœ¬åœ°é€‰é¡¹

### Ollama

```bash
# .env
SAFEGUARD_BACKEND=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# å®‰è£… Ollama: https://ollama.ai/
ollama run llama3
```

### è‡ªæ‰˜ç®¡ API

GuardClaw å…¼å®¹ä»»ä½• OpenAI-compatible API ç«¯ç‚¹ã€‚

## æ¨èé…ç½®

**å¼€å‘ç¯å¢ƒï¼ˆé€Ÿåº¦ä¼˜å…ˆï¼‰ï¼š**

```bash
SAFEGUARD_BACKEND=lmstudio
LMSTUDIO_MODEL=phi-2
```

**ç”Ÿäº§ç¯å¢ƒï¼ˆå‡†ç¡®åº¦ä¼˜å…ˆï¼‰ï¼š**

```bash
SAFEGUARD_BACKEND=lmstudio
LMSTUDIO_MODEL=mistral-7b-instruct
```

**ä½èµ„æºè®¾å¤‡ï¼š**

```bash
SAFEGUARD_BACKEND=fallback  # ä½¿ç”¨æ¨¡å¼åŒ¹é…
```

## æç¤º

1. **é¦–æ¬¡åŠ è½½æ…¢**ï¼šç¬¬ä¸€æ¬¡åˆ†æä¼šåŠ è½½æ¨¡å‹ï¼Œéœ€è¦ 10-30 ç§’
2. **æ‰¹é‡åˆ†æ**ï¼šLM Studio æ”¯æŒå¹¶å‘è¯·æ±‚
3. **å¤šæ¨¡å‹åˆ‡æ¢**ï¼šå¯ä»¥åœ¨ LM Studio ä¸­å¿«é€Ÿåˆ‡æ¢æ¨¡å‹æµ‹è¯•æ•ˆæœ
4. **GPU åŠ é€Ÿ**ï¼šåœ¨ LM Studio è®¾ç½®ä¸­å¯ç”¨ GPU offloading å¯æ˜¾è‘—æé€Ÿ

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** æ£€æŸ¥ GuardClaw æ—¥å¿—ï¼š

```bash
tail -f ~/clawd/guardclaw/guardclaw.log
```
